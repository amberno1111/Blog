# Java内存模型

Java Memory Model即Java内存模型，简称JMM，根据[JSR-133: Java Memory Model and Thread Specification](http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf)中的描述，它是一个跟多线程相关的概念。

所以在开始介绍JMM之前，先来了解几个跟多线程相关的概念。

## 一些前置概念

在并发编程中需要处理两个关键的问题：
- 线程之间如何通信
- 线程之前如何同步

**通信指的是线程之间以什么机制来交换信息**，一般来说有两种通信机制:
- 共享内存：线程之间通过读、写内存中的公共状态来隐式的进行通信，Java采用的就是这种模型
- 消息传递：线程之间使用显式的发送消息来进行通信

**同步指的用于控制不同线程之间的操作发生相对顺序的机制**，也分为两种情况：
- 在共享内存模型中，线程之间的同步是显式的，需要程序员显式的编写代码来指定某些代码块或者方法需要在线程之间互斥执行。
- 在消息传递模型中，由于消息的发送必定在消息的接收之前，因此同步是隐式进行的。


## JMM抽象结构

在Java中，所有的实例域、静态域和数组元素都存储在堆内存当中，堆内存在线程之间共享，这部分数据(本文称之为共享变量)受到JMM的控制，因为这些数据是会有内存可见性问题的。而其他的数据比如局部变量，这些数据不会在线程之间共享，所以不会有内存可见性问题，也不受JMM的影响。

Java线程之间的通信受到JMM的控制，**JMM决定一个线程对共享变量写入何时对另一个线程可见**。下图是一个抽象的JMM结构：

![JMM](https://user-images.githubusercontent.com/16413289/60766831-bc5fee00-a0e1-11e9-8de8-cc4d543ac228.jpg)

其中，
- **本地内存(Local Memory)存储了该线程读/写共享变量的副本，但它实际上是一个抽象的概念，涵盖了缓存、写缓冲区、寄存器以及重排序等硬件和编译器的优化。**
- 主内存就是真实的内存

从上图来看，JMM是通过控制共享变量何时刷新到主内存来控制线程A与线程B之间的通信。具体流程是这样的：
- 假设在主内存中存在一个共享变量`i=0`
- 线程A的本地内存有一个共享变量的副本`i=0`，然后线程A进行了写入操作，其本地内存中的数据就变成了`i=1`
- 但是这个时候主内存中的变量值依然是`i=1`，如果线程B从主内存中直接读的话，读到的肯定是`i=0`；而且线程B的本地内存中已经有可能有`i=0`的缓存副本了，这时候线程B继续使用`i`的值进行操作就会出现错误的结果
- 这时候JMM就会发挥作用，JMM通过让线程A主动把`i=1`这个写入更新到主内存，然后告诉线程B不能从缓存中取共享变量的数据，必须从主内存中读，这样线程B就可以正确从主内存中读取到`i=1`了
- 这整个过程相当于线程A直接给线程B发消息了，这就是共享内存中如何进行通信的一个示例

**从上面的流程中我们可以看到，JMM本质上是通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证的。**

## 顺序一致性模型

Java程序员在写代码的时候，肯定有一个统一的幻觉，那就是代码指令肯定是按照程序员编写好的顺序执行的，这就是顺序一致性模型，它是一个被计算机科学家理想化了的模型。实际上，由于编译器重排序，处理器指令并行技术以及缓存、写缓冲区带来的重排序效果等等因素的影响，程序员编写的代码指令并非会按照编写好的顺序执行。

ps：编译器重排序、处理器指令并行技术以及缓存、写缓冲区带来的重排序效果的意思是这三种情况都会乱序执行代码指令，比如代码顺序是`a=1; b=2`，但是真正到CPU执行的时候，执行顺序可能是`b=2; a=1`，这个顺序有可能是由上面所说的任意三个因素引起的。例子里的这个重排序看起来不会导致代码执行结果不一样，但是某些重排序就可能会引起执行结果不一致，具体的可以看后面重排序相关的章节。这里只需要记住，JMM使用了一些手段来对付这些重排序，使得单线程或正确同步的程序代码指令执行之后能够得到正确的结果。

但是，顺序一致性模型是处理器内存模型以及JMM这一类语言级别的内存模型的参照对象，**JMM对于单线程程序以及正确同步的多线程程序做了如下保证：单线程程序或者被正确同步的多线程程序，程序的执行结果与该程序在顺序一致性模型中执行的结果是一致的**。也就是说，JMM不保证程序指令是按顺序执行的，但是它能在单线程程序或者被正确同步的多线程程序中保证**程序的执行结果与顺序执行的执行结果是一致的**。

ps：JMM提供了一些同步原语(`volatile`、`synchronized`、`final`)来实现多线程之间的正确同步。

顺序一致性模型有两个特性：
- 一个线程中所有的操作必须按照程序的顺序来执行
- (不管是否正确同步)所有的线程都只能看到一个单一的操作执行顺序。在顺序一致性模型中，每个操作都必须是原子执行并且立刻对多有线程可见

下面举两个例子来说明顺序一致性模型的特性：

### 单线程或者正确同步的多线程程序

假设线程A有三个按顺序排好的操作：A1，A2，A3；
那么在单线程程序下，执行的顺序就是A1，A2，A3，符合前面说的两个特性。

现在再加一个线程B，也有三个操作：B1、B2、B3；然后正确同步使得B1必须在A3之后执行，那么执行顺序就变成了：A1，A2，A3，B1，B2，B3；

这时候操作整体上有序，而且两个线程都能看到这个执行顺序。

### 未正确同步的多线程程序

假设线程A和线程B之间没有正确同步，那么执行就是乱序的，可能是：A1，B1，B2，B3，A2，A3。
虽然执行的结果可能不是我们想要的，但是还是能保证符合上面所说的两个特性。两个线程所看到的执行顺序也都是一致的。

### JMM vs 顺序一致性模型

对于顺序一致性模型的两个特性，JMM没有做保证。**JMM的设计与实现方针是，在不改变(单线程or正确同步多线程)程序执行结果的前提下，尽量的允许编译器和硬件对代码指令做各种重排序的优化。**

对于未正确同步的多线程程序，JMM只提供最小安全性，就是读取变量的时候这个变量不会不存在，而且会读取到上次的值或者是默认值，只要程序不报错就行了。为了实现最小安全性，JMM会要求虚拟机在堆上分配对象时，首先对内存空间清零，然后才会在上面分配对象，并且需要保证这两个操作时正确的同步的。

所以总结以下JMM和顺序一致性模型的差异：
- 顺序一致性模型保证单线程内的操作会按照程序的顺序执行，而JMM不保证
- 顺序一致性模型保证所有的线程都能看到一致的操作执行顺序，而JMM不保证
- JMM不保证对64bit的long和double型变量的写具有原子性，而顺序一致性模型保证对所有变量的读/写操作都具有原子性

第三个差异是由于硬件的总线事务引起的，在32bit机器上，64bit的long和double都是用两个32bit来表示的，就可能一个总线事务写前面的32bit，另一个总线事务写后面的32bit，总线事务的并发可能会导致出错。所以JMM鼓励但不强求这两种数据类型的写原子性，而且在32bit的机器上实现64bit变量的写原子性性能开销比较大。

## 重排序

在执行程序时，编译器和处理器为了提高性能，往往会乱序执行指令：
- 编译器会对代码进行重排序，这个重排序本质上和CPU指令重排序是一致的
- CPU指令并行技术ILP会对CPU指令进行重排序
- CPU使用了写缓冲区，在读写内存的时候，会出现类似于指令重排序的效果

在单线程或者单核环境下，有些指令是肯定不会被重排序的，那就是存在数据依赖的指令，比如`a=1; b=a+1`，这两个指令在编译器和CPU重排序中肯定不会出现。

### 写缓冲区导致的重排序

现代CPU使用了写缓冲区临时保存需要写入到内存的数据。写缓冲区有以下优势：
- 可以保证指令流水线的持续运行，避免CPU停顿下来等待向内存写入数据而产生的延迟
- 可以把数据批量写向内存，提高写入效率

**CPU向写缓冲区写数据，并且批量刷新到内存**这个特性会对内存操作的执行顺序产生影响，即处理器对内存的读/写操作的执行顺序与真实在内存中发生的读/写顺序不一致！因为每个CPU只能访问自己的写缓冲区。

来看一个例子：
假设CPU_A执行以下两条指令：A1(a = 1)，A2(x = b)；CPU_B执行两条指令：B1(b = 2)，B2(y = a)；在主内存中的初始状态：a=b=0。执行完毕之后，可能会出现x=y=0的情况。来看一下执行顺序：
- 首先CPU_A向写缓冲区写a=1，但是没有写回到主内存
- 然后CPU_B向写缓冲区写b=2，但是没有写回到主内存
- CPU_A从主内存中读取b=0，执行x=b之后得到x=0
- CPU_B从主内存中读取a=0，执行y=a之后得到y=0

完全得到了一个错误的结果，从这个执行流程看来，相当于重排序了操作A1和A2，也重排序了操作B1和B2。这完全改变了多线程环境下程序的语义，JMM需要找到合适的机制避免这种重排序。

### 编译器代码重排序和CPU指令重排序

来看一段代码：
```java
public class ReorderDemo {
    int a = 0;
    boolean flag = false;

    public void write() {
        a = 1;  // 操作 1
        flag = true; // 操作 2
    }

    public void read() {
        if (flag) { // 操作 3
            int b = a * 2; // 操作 4
        }
    }
}
```

#### 编译器代码重排序

先来看操作1和操作2，由于两个操作没有数据依赖，所以编译器会对这两个操作进行重排序。在多线程环境下，假设线程A执行`write`方法，线程B执行`read`方法：
- 由于编译器进行的重排序，线程A先执行了操作2，`flag=true`
- 然后线程B执行了操作3和操作4，得到`b=0*2=0`
- 最后线程A执行了`a=1`

这种情况下得到的结果也是不对的，说明**多线程环境下，编译器代码重排序也会改变程序的语义！**

#### CPU指令重排序

继续来看操作3和操作4，这两其实是有控制依赖性的，所以编译器一般不会对这两个操作进行重排序，但是CPU会！！为了改善性能，CPU会采用猜测执行来克服控制相关性对并行度的影响，CPU会先执行操作4，然后把结果存到重排序缓冲区，然后当flag为true的时候，就把缓冲区的值取出来，并且复制给b。这种情况下得到的结果依然是`b=0`，说明**多线程环境下，CPU指令重排序也会改变程序的语义！**


## JMM是如何解决重排序问题的

在开始之前，依然先来看几个概念：
- as-if-serial：这个概念的语义是不管怎么重排序(编译器和CPU)，(单线程)程序的执行结果不能被改变。
- happens-before：JMM用于表示两个操作之间的内存可见性的概念，比如A happens-before B，表示操作A的执行结果必须对B可见。在JMM里，代码顺序也可以产生happens-before规则，比如操作A的代码写在操作B之前，也称为A happens-before B。JMM会禁止某些符合happens-before规则的指令重排序。

### JMM的设计

对于现代的编译器和处理器来说，他们希望内存模型的束缚越小越好，这样就可以尽可能的通过提高指令并行度来改善性能；但是对于程序员来说，希望内存模型能够提供非常强的内存可见性保证，然而这种强内存可见性跟重排序是冲突的。**所以JMM设计的核心目标就是找到一个好的平衡点：一方面要为程序员提供足够强的内存可见性保证，这样代码才好写；另一方面对编译器和处理器的限制要尽可能的放松，让他们做优化以提高性能。**

先看一段代码：
```java
public double calArea() {
    double pi = 3.14;    // 操作 A
    double r = 3;       // 操作 B
    return r * r * pi;  // 操作 C
}
```
使用JMM定义的happens-before规则定义可以得到：A happens-before B，B happens-before C，A happens-before C。
但是我们很明显可以看到，其实A和B是可以重排序的，因为他们没有数据依赖性，而且即使有A happens-before B的规则存在，JMM依然允许对这种代码进行重排序。所以其实JMM把带有happens-before关系的重排序分为两类：
- 会改变程序执行结果的重排序，比如上面的操作A和操作C，或者操作B和操作C
- 不会改变程序执行结果的重排序，比如上面的操作A和操作B

对于这两种重排序，JMM有不同的策略，对于会改变程序结果的重排序，JMM会要求编译器和处理器禁止这种重排序；对于不会改变结果的重排序，JMM不做任何处理。也就是说**JMM认为只要不改变程序的执行结果，代码或者指令可以随意的重排序。**

另外，JMM的happens-before规则同时也给程序员一个非常强的内存可见性保证，只要遵守JMM定义的happens-before规则，程序员编写的代码的执行结果就能跟在内存一致性模型中执行的结果一致。

#### JMM定义的happens-before规则

[JSR-133: Java Memory Model and Thread Specification](http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf)定义了以下这些happens-before规则：

- 程序顺序规则：一个线程中的每个操作，都happens-before这个操作之后的所有操作
- 监视器锁规则：监视器锁的解锁happens-before随后对这个监视器锁的加锁
- `volatile`变量规则：对一个`volatile`变量的写happens-before随后对这个变量的读
- 传递性：如果A happens-before B，B happens-before C，那么A happens-before C
- `start()`规则：如果线程A执行`ThreadB.start()`启动线程B，那么A线程的`Thread.start()`操作happens-before线程B里所有的操作
- `join()`规则：如果线程A执行`ThreadB.join()`并成功返回，那么线程B中的任意操作happens-before线程A的`ThreadB.join()`

第一条程序顺序规则很简单，这里就不多做介绍了。

##### 监视器锁规则与`synchronized`内存语义

我们再来看一边上面例子里出现的代码：
```java
public class ReorderDemo {
    int a = 0;
    boolean flag = false;

    public void write() {
        a = 1;  // 操作 1
        flag = true; // 操作 2
    }

    public void read() {
        if (flag) { // 操作 3
            int b = a * 2; // 操作 4
        }
    }
}
```

JMM应该怎么做才能避免这个代码出现问题呢？很简单，给操作2和操作3加上happens-before规则。
这样上述代码的happens-before规则就变成了：
- 操作1 happens-before 操作2
- 操作2 happens-before 操作3
- 操作3 happens-before 操作4
- 根据传递性：操作1 happens-before 4，这样就可以完美的避免`int b = a * 2`的执行在`a=1`的执行之前的情况了，因为这两条指令是有数据依赖的，所以这种happens-before规则在JMM看来是需要禁止重排序的。

监视器锁规则就可以用来给操作2和操作3增加happens-before规则，`synchronized`关键字就是JVM根据JMM的规范所实现的监视器锁，修改以后的代码是这样的(Java里的锁出现的原因就是这个监视器锁规则)：
```java
public class ReorderDemo {
    int a = 0;
    boolean flag = false;

    public synchronized void write() {
        a = 1;  // 操作 1
        flag = true; // 操作 2
    }

    public synchronized  void read() {
        if (flag) { // 操作 3
            int b = a * 2; // 操作 4
        }
    }
}

```

监视器锁的规则限定了几个操作的happens-before规则：
- 操作2 happens-before 操作3：所以线程A执行完操作2，释放锁之后，线程B执行操作3时，操作2的执行结果必须对线程B可见。也就是这里肯定有两个操作，一是线程A释放锁的时候，必须把本地内存中的flag变量刷新到主内存；二是线程B获取锁的时候，必须把本地内存中的flag变量设置为失效，然后从主内存中重新读取flag变量的值
- 操作1 happens-before 4：这个其实和上面的flag变量的更新是一样的，就不展开说了


`synchronized`的内存语义其实就是锁的释放和获取的内存语义：
- 锁的释放的内存语义是：当线程释放锁的时候，JMM会把线程本地内存中的共享变量刷新到主内存中
- 锁的获取的内存语义是：当线程获取锁时，JMM会把线程本地内存中的共享变量设置为无效，从主内存中重新读取共享变量

##### `volatile`规则与`volatile`的内存语义

还是同一个例子：
```java
public class ReorderDemo {
    int a = 0;
    boolean flag = false;

    public void write() {
        a = 1;  // 操作 1
        flag = true; // 操作 2
    }

    public void read() {
        if (flag) { // 操作 3
            int b = a * 2; // 操作 4
        }
    }
}
```

想要得到正确的结果，其实我们只需要操作1 happens-before 操作4就可以了。`volatile`规则就可以做到，代码修改为如下：
```java
public class ReorderDemo {
    int a = 0;
    volatile boolean flag = false;

    public void write() {
        a = 1;  // 操作 1
        flag = true; // 操作 2
    }

    public void read() {
        if (flag) { // 操作 3
            int b = a * 2; // 操作 4
        }
    }
}
```

因为`volatile`规则定义的是`volatile`写happens-before`volatile`读，那么操作2 happens-before 操作4，也就是说，`a=1`的写操作永远在`int b=a*2`的读操作之前，那么最后得到的`b`的值就是正确的。

实际上，`volatile`变量的写/读具有跟监视器锁解锁/加锁同样的内存语义：
- `volatile`变量的写和监视器锁的解锁有相同的内存语义：`volatile`变量写完成时，JMM会把该线程对应的本地内存中的`volatile`共享变量值刷新到主内存
- `volatile`变量的读和监视器锁的加锁有相同的内存语义：`volatile`变量读开始时，JMM会把该线程对应的本地内存中的`volatile`共享变量设置为无效，然后从主内存中读取这个变量的值

### JMM如何实现所定义的内存语义

这一章节主要讲JMM如果实现它所定义的内存语义，即如何实现`volatile`和`synchronized`所对应的内存语义。

**JMM通过规定Java编译器在生成指令序列的适当位置插入内存屏障指令来禁止特定类型的重排序，从而实现`volatile`和`synchronized`的内存语义。**

JMM把内存屏障分为四类：
- `LoadLoad Barriers`：`Load1; LoadLoad; Load2;`，确保`Load1`的数据装载优先于`Load2`以及后续所有的数据装载
- `StoreStore Barriers`：`Store1; StoreStore; Store2;`，确保`Store1`的数据写入对其他处理器可见(刷新到主内存)优先于`Store2`以及后续所有的数据存储
- `LoadStore Barriers`：`Load1; LoadStore; Store2;`，确保`Load1`的数据装载优先于`Store2`以及后续所有的数据存储
- `StoreLoad Barriers`：`Store1; StoreLoad; Load2;`，确保`Store1`的数据写入对其他处理器可见(刷新到主内存)优先于`Load2`以及后续所有的数据装载。`StoreLoad`会使该屏障之前所有的内存访问指令(存储和装载指令)完成之后，才执行该屏障之后的内存访问指令。所以这是一个全能型内存屏障，当然也是最耗费性能的

#### `volatile`内存语义的实现

`volatile`的内存语义是：`volatile`变量的写happens-before`volatile`变量的读。从这个内存语义中，我们总结出几条规则：
- 当第一个操作是`volatile`写，第二个操作是`volatile`读时，不能重排序
- 当第二个操作是`volatile`写，不管第一个操作是什么，都不能重排序。因为`volatile`写的这个值，可能来自于前面的普通变量的读写或者`volatile`变量的读写，如果这些操作有被重排序到后面，就很麻烦了
- 当第一个操作时`volatile`读时，后面的操作不管是什么，都不能重排序

JMM会采取保守策略来插入内存屏障：
- 在每个`volatile`写操作前面插入一个`StoreStore`屏障
- 在每个`volatile`写操作后面插入一个`StoreLoad`屏障
- 在每个`volatile`读操作后面插入一个`LoadLoad`屏障
- 在每个`volatile`读操作后面插入一个`LoadStore`屏障

**JMM非常严格的确保了`volatile`变量的写/读与锁的释放/加锁拥有相同的内存语义！**

#### 锁内存语义的实现

锁的内存语义其实是利用了`volatile`实现的，比如可以参考`ReentrantLock`里的公平锁，是这么做的：
- 加锁的时候先读一个`volatile`变量`state`
- 释放锁的时候写一个`volatile`变量`state`

`ReentrantLock`里的非公平锁更有意思一些，它的释放锁与公平锁完全一样，但是它的加锁是通过CAS操作来实现的，**CAS操作同时具有`volatile`读和写的内存语义。**

因为CAS操作调用的是CPU所提供的`Lock`指令，而这个指令可以保证在内存中对一个变量进行`读-改-写`是一个原子操作。

#### concurrent包的一个通用化实现模式

concurrent包里的很多原子操作、多线程安全的操作、线程之间安全通信机制等等都是通过CAS配合`volatile`变量的读/写来实现的，可以概括为一个通用模式，比如AQS就是这么做的：
- 声明一个共享变量为`volatile`
- 使用CAS的原子条件更新来实现线程之间的同步
- 配合`volatile`读/写和CAS的内存语义来实现线程之间的通信


